{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/daviderickson/projects/datasf/data/Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Address', 'X', 'Y', 'Location', 'PdId',\n",
    "       'SF Find Neighborhoods', 'Current Police Districts',\n",
    "       'Current Supervisor Districts', 'Analysis Neighborhoods']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_list = ['Address', 'X', 'Y', 'Location', 'PdId',\n",
    "       'SF Find Neighborhoods', 'Current Police Districts',\n",
    "       'Current Supervisor Districts', 'Analysis Neighborhoods']\n",
    "for col in cols_list:\n",
    "    series = df[col]\n",
    "    series.unique()\n",
    "    print(col, '- unique entries - ', len(series.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns: \n",
    "    series = df[col]\n",
    "    print(len(series.unique()), col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Category')['DayOfWeek'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Resolution').count()['IncidntNum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_dow = df.pivot_table('IncidntNum', index='Resolution', columns='DayOfWeek', aggfunc='count', margins=False)\n",
    "df_res_dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow_list = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "df_res_dow['total'] = df_res_dow.sum(axis=1)\n",
    "for col in df_res_dow.columns:\n",
    "    df_res_dow[col] = df_res_dow[col] / df_res_dow['total']\n",
    "df_res_dow.drop('total', axis=1, inplace=True)\n",
    "df_res_dow = df_res_dow[dow_list]\n",
    "df_res_dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "ax = sns.heatmap(df_res_dow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_dow = df.pivot_table('IncidntNum', index='Category', columns='DayOfWeek', aggfunc='count', margins=False)\n",
    "df_cat_dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow_list = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "df_cat_dow['total'] = df_cat_dow.sum(axis=1)\n",
    "for col in df_cat_dow.columns:\n",
    "    df_cat_dow[col] = df_cat_dow[col] / df_cat_dow['total']\n",
    "df_cat_dow.drop('total', axis=1, inplace=True)\n",
    "df_cat_dow = df_cat_dow[dow_list]\n",
    "df_cat_dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,8))\n",
    "ax = sns.heatmap(df_cat_dow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table('IncidntNum', index='Resolution', columns='Category', aggfunc='count', margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('DayOfWeek')['DayOfWeek'].count().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''times = pd.to_datetime(df['Time'])\n",
    "df['times'] = times'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df.pivot_table('IncidntNum', index='DayOfWeek', columns='Category', aggfunc='count')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''vals, bins, patches = plt.hist(df['DayOfWeek'])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca = PCA()\n",
    "print(pca)\n",
    "\n",
    "print(df.shape)\n",
    "    \n",
    "dummies_cols = ['Category', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution']\n",
    "drop_cols = ['Date', 'Time', 'Address', 'Location'] #Date and Tme can be handled better\n",
    "df1 = pd.get_dummies(df, columns=dummies_cols)\n",
    "df1 = df1.drop(drop_cols, axis=1)\n",
    "df1 = df1.dropna(axis=0)\n",
    "X = df1.loc[:,:].values\n",
    "X_cols = df1.loc[:,:].columns\n",
    "\n",
    "scaler=StandardScaler()\n",
    "X_transformed = scaler.fit_transform(X)\n",
    "\n",
    "pca.fit(X_transformed)\n",
    "X_pca = pca.fit(X_transformed).transform(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained Variance \n",
    "plt.figure(figsize=(15,8))\n",
    "ax = sns.barplot(x=np.arange(100), y=pca.explained_variance_[0:100]/sum(pca.explained_variance_))\n",
    "ax.set_xlabel('Principal Component')\n",
    "ax.set_ylabel('Explained Variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of explained variance\n",
    "vals, bins, patches = plt.hist(pca.explained_variance_/sum(pca.explained_variance_), log=True, bins=50)\n",
    "plt.title('Histogram of explained variance')\n",
    "plt.xlabel('Explained Variance')\n",
    "plt.ylabel('Number of components')\n",
    "\n",
    "# Observations\n",
    "num_exp_var = np.sum(vals)-vals[0] #\n",
    "print(num_exp_var, 'PCA components explain all variance.')\n",
    "print(np.sum(vals), 'total features')\n",
    "print(num_exp_var/np.sum(vals), '% of all features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Category'] == 'LARCENY/THEFT', 'Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[df.dtypes == 'object'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def make_Xy_RF(df, y='Resolution'): \n",
    "    drop_cols = ['IncidntNum', 'Location'] \n",
    "    df1 = df.drop(drop_cols, axis=1)\n",
    "    df1.loc[df1['Category'] == 'LARCENY/THEFT', 'Category'] = 'Larceny_Theft'\n",
    "    # df1 = df1.dropna(axis=0)\n",
    "    dummies_cols = df1.columns[df1.dtypes == 'object'].to_list()\n",
    "    df1 = pd.get_dummies(df1, columns=dummies_cols)\n",
    "    X = df1.loc[:,:].values\n",
    "    X_cols = df1.loc[:,:].columns\n",
    "    y = df['Resolution']\n",
    "    return X, y, X_cols\n",
    "\n",
    "X, y, X_cols = make_Xy_RF(df, y='Resolution')\n",
    "model = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "\n",
    "param_range=[4,5,6,8,10,13,20,30,40,50,60,100]\n",
    "train_scores_vc, test_scores_vc = \\\n",
    "    validation_curve(estimator=model, X=X, y=y, param_name=\"n_estimators\", param_range=param_range, \\\n",
    "                     cv=10, n_jobs=-1, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores_vc, axis=1)\n",
    "train_scores_std = np.std(train_scores_vc, axis=1)\n",
    "test_scores_mean = np.mean(test_scores_vc, axis=1)\n",
    "test_scores_std = np.std(test_scores_vc, axis=1)\n",
    "\n",
    "ax = plt.figure()\n",
    "plt.plot(param_range,train_scores_mean,'o',label=\"Train\")\n",
    "plt.plot(param_range,test_scores_mean,'o',label=\"Test\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Validation Curves\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xscale('log')\n",
    "ax.fill_between(param_range, train_scores_mean+train_scores_std, train_scores_mean+train_scores_std, \n",
    "                facecolor='blue', alpha=0.5)\n",
    "ax.fill_between(param_range, test_scores_mean+test_scores_std, test_scores_mean+test_scores_std, \n",
    "                facecolor='red', alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
